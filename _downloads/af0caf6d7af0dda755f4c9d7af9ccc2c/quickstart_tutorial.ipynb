{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8oC6mR0Ckrx",
        "outputId": "ece551a6-b8b8-4cf6-b9a7-451c4e339294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296945  [   64/60000]\n",
            "loss: 2.295526  [ 6464/60000]\n",
            "loss: 2.271598  [12864/60000]\n",
            "loss: 2.270846  [19264/60000]\n",
            "loss: 2.248022  [25664/60000]\n",
            "loss: 2.225307  [32064/60000]\n",
            "loss: 2.234571  [38464/60000]\n",
            "loss: 2.199884  [44864/60000]\n",
            "loss: 2.198545  [51264/60000]\n",
            "loss: 2.172892  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 2.159579 \n",
            "\n",
            "Done!\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.164999  [   64/60000]\n",
            "loss: 2.163653  [ 6464/60000]\n",
            "loss: 2.100068  [12864/60000]\n",
            "loss: 2.115119  [19264/60000]\n",
            "loss: 2.066213  [25664/60000]\n",
            "loss: 2.014482  [32064/60000]\n",
            "loss: 2.035126  [38464/60000]\n",
            "loss: 1.959572  [44864/60000]\n",
            "loss: 1.972009  [51264/60000]\n",
            "loss: 1.890944  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.9%, Avg loss: 1.885920 \n",
            "\n",
            "Done!\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.922787  [   64/60000]\n",
            "loss: 1.895638  [ 6464/60000]\n",
            "loss: 1.772443  [12864/60000]\n",
            "loss: 1.803803  [19264/60000]\n",
            "loss: 1.698431  [25664/60000]\n",
            "loss: 1.658675  [32064/60000]\n",
            "loss: 1.670026  [38464/60000]\n",
            "loss: 1.576102  [44864/60000]\n",
            "loss: 1.609576  [51264/60000]\n",
            "loss: 1.492178  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 1.511687 \n",
            "\n",
            "Done!\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.586994  [   64/60000]\n",
            "loss: 1.554229  [ 6464/60000]\n",
            "loss: 1.401986  [12864/60000]\n",
            "loss: 1.459188  [19264/60000]\n",
            "loss: 1.349287  [25664/60000]\n",
            "loss: 1.352106  [32064/60000]\n",
            "loss: 1.357437  [38464/60000]\n",
            "loss: 1.286338  [44864/60000]\n",
            "loss: 1.328447  [51264/60000]\n",
            "loss: 1.225075  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.249414 \n",
            "\n",
            "Done!\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.333232  [   64/60000]\n",
            "loss: 1.318411  [ 6464/60000]\n",
            "loss: 1.150677  [12864/60000]\n",
            "loss: 1.245629  [19264/60000]\n",
            "loss: 1.126555  [25664/60000]\n",
            "loss: 1.154790  [32064/60000]\n",
            "loss: 1.170449  [38464/60000]\n",
            "loss: 1.109545  [44864/60000]\n",
            "loss: 1.155301  [51264/60000]\n",
            "loss: 1.073360  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 1.089745 \n",
            "\n",
            "Done!\n",
            "Saved PyTorch Model State to model.pth\n",
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    # print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    # print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "# print(model)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "\n",
        "\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "#print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "#print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "#print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "#print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "#print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
        "\n",
        "tensor = torch.rand(3,4)\n",
        "\n",
        "#print(f\"Shape of tensor: {tensor.shape}\")\n",
        "#print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "#print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "#print(f\"First row: {tensor[0]}\")\n",
        "#print(f\"First column: {tensor[:, 0]}\")\n",
        "#print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "#print(tensor)\n",
        "\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "# print(t1)\n",
        "\n",
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "#print(torch.matmul(tensor, tensor.T, out=y3))\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "#print(torch.mul(tensor, tensor, out=z3))\n",
        "\n",
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "# print(agg_item, type(agg_item))\n",
        "\n",
        "#print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "#print(tensor)\n",
        "\n",
        "t = torch.ones(5)\n",
        "#print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "#print(f\"n: {n}\")\n",
        "\n",
        "t.add_(1)\n",
        "#print(f\"t: {t}\")\n",
        "#print(f\"n: {n}\")\n",
        "\n",
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "id": "YdC7QaZsHbdb",
        "outputId": "b9e74c43-9db7-487f-cfff-b1e358537d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "# plt.show()\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "VYX0NWnuHtS1",
        "outputId": "f42948e5-0255-4dcc-bd98-31e619c50b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")\n",
        "\n",
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
        "\n",
        "print(target_transform)"
      ],
      "metadata": {
        "id": "FJl6ZkR2L0P4",
        "outputId": "ae59b608-d45a-48b3-9c78-67b24fdf8e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambda()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "#print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "#print(model)\n",
        "\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "#print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "input_image = torch.rand(3,28,28)\n",
        "#print(input_image.size())\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "#print(flat_image.size())\n",
        "\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "#print(hidden1.size())\n",
        "\n",
        "#print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "#print(f\"After ReLU: {hidden1}\")\n",
        "\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "id": "sRp5hlL_Mup9",
        "outputId": "67632200-8e6c-4aaa-da0e-57901cd71975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0179, -0.0188, -0.0352,  ...,  0.0164,  0.0120,  0.0079],\n",
            "        [ 0.0195, -0.0308,  0.0261,  ...,  0.0147,  0.0148, -0.0179]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0231, 0.0286], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0164,  0.0041, -0.0148,  ..., -0.0385,  0.0394, -0.0281],\n",
            "        [ 0.0033,  0.0286, -0.0414,  ..., -0.0220, -0.0169, -0.0248]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0398,  0.0087], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0121,  0.0253,  0.0285,  ..., -0.0325,  0.0429, -0.0077],\n",
            "        [ 0.0225, -0.0041, -0.0266,  ..., -0.0060, -0.0154,  0.0062]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0219,  0.0390], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "\n",
        "#print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "#print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "loss.backward()\n",
        "#print(w.grad)\n",
        "#print(b.grad)\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "#print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "#print(z.requires_grad)\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "id": "ULKvGMr7PjVo",
        "outputId": "11327316-f9d2-412e-ab52-03c8874af19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "ZyheFWCjQ8mZ",
        "outputId": "5ae3a0d1-0832-4d8a-8669-4d45aa9605da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298529  [   64/60000]\n",
            "loss: 2.290851  [ 6464/60000]\n",
            "loss: 2.269262  [12864/60000]\n",
            "loss: 2.272489  [19264/60000]\n",
            "loss: 2.240981  [25664/60000]\n",
            "loss: 2.215873  [32064/60000]\n",
            "loss: 2.229251  [38464/60000]\n",
            "loss: 2.193471  [44864/60000]\n",
            "loss: 2.192701  [51264/60000]\n",
            "loss: 2.156981  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.6%, Avg loss: 2.151651 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.165715  [   64/60000]\n",
            "loss: 2.153668  [ 6464/60000]\n",
            "loss: 2.090794  [12864/60000]\n",
            "loss: 2.116464  [19264/60000]\n",
            "loss: 2.059402  [25664/60000]\n",
            "loss: 1.986673  [32064/60000]\n",
            "loss: 2.031722  [38464/60000]\n",
            "loss: 1.943611  [44864/60000]\n",
            "loss: 1.954394  [51264/60000]\n",
            "loss: 1.879538  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.2%, Avg loss: 1.878007 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.913633  [   64/60000]\n",
            "loss: 1.881138  [ 6464/60000]\n",
            "loss: 1.759573  [12864/60000]\n",
            "loss: 1.816750  [19264/60000]\n",
            "loss: 1.707585  [25664/60000]\n",
            "loss: 1.638463  [32064/60000]\n",
            "loss: 1.682993  [38464/60000]\n",
            "loss: 1.575767  [44864/60000]\n",
            "loss: 1.606229  [51264/60000]\n",
            "loss: 1.502564  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.8%, Avg loss: 1.520205 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.585810  [   64/60000]\n",
            "loss: 1.553502  [ 6464/60000]\n",
            "loss: 1.399668  [12864/60000]\n",
            "loss: 1.486089  [19264/60000]\n",
            "loss: 1.371740  [25664/60000]\n",
            "loss: 1.352417  [32064/60000]\n",
            "loss: 1.379974  [38464/60000]\n",
            "loss: 1.301363  [44864/60000]\n",
            "loss: 1.334941  [51264/60000]\n",
            "loss: 1.237784  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.261367 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.336727  [   64/60000]\n",
            "loss: 1.323056  [ 6464/60000]\n",
            "loss: 1.150629  [12864/60000]\n",
            "loss: 1.268959  [19264/60000]\n",
            "loss: 1.146368  [25664/60000]\n",
            "loss: 1.161444  [32064/60000]\n",
            "loss: 1.189526  [38464/60000]\n",
            "loss: 1.125641  [44864/60000]\n",
            "loss: 1.163078  [51264/60000]\n",
            "loss: 1.079270  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.0%, Avg loss: 1.096635 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.167055  [   64/60000]\n",
            "loss: 1.175173  [ 6464/60000]\n",
            "loss: 0.982847  [12864/60000]\n",
            "loss: 1.129887  [19264/60000]\n",
            "loss: 1.004363  [25664/60000]\n",
            "loss: 1.028268  [32064/60000]\n",
            "loss: 1.068273  [38464/60000]\n",
            "loss: 1.009302  [44864/60000]\n",
            "loss: 1.049297  [51264/60000]\n",
            "loss: 0.977380  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.2%, Avg loss: 0.987795 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.045822  [   64/60000]\n",
            "loss: 1.076914  [ 6464/60000]\n",
            "loss: 0.865420  [12864/60000]\n",
            "loss: 1.035748  [19264/60000]\n",
            "loss: 0.913476  [25664/60000]\n",
            "loss: 0.932402  [32064/60000]\n",
            "loss: 0.987711  [38464/60000]\n",
            "loss: 0.931334  [44864/60000]\n",
            "loss: 0.969982  [51264/60000]\n",
            "loss: 0.908509  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.6%, Avg loss: 0.912746 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.955383  [   64/60000]\n",
            "loss: 1.007643  [ 6464/60000]\n",
            "loss: 0.780366  [12864/60000]\n",
            "loss: 0.968581  [19264/60000]\n",
            "loss: 0.853367  [25664/60000]\n",
            "loss: 0.861885  [32064/60000]\n",
            "loss: 0.931032  [38464/60000]\n",
            "loss: 0.878469  [44864/60000]\n",
            "loss: 0.912812  [51264/60000]\n",
            "loss: 0.859316  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.858732 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.885642  [   64/60000]\n",
            "loss: 0.955420  [ 6464/60000]\n",
            "loss: 0.716476  [12864/60000]\n",
            "loss: 0.918723  [19264/60000]\n",
            "loss: 0.811240  [25664/60000]\n",
            "loss: 0.809194  [32064/60000]\n",
            "loss: 0.888433  [38464/60000]\n",
            "loss: 0.841759  [44864/60000]\n",
            "loss: 0.870493  [51264/60000]\n",
            "loss: 0.822253  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.818099 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.830128  [   64/60000]\n",
            "loss: 0.913616  [ 6464/60000]\n",
            "loss: 0.667195  [12864/60000]\n",
            "loss: 0.880257  [19264/60000]\n",
            "loss: 0.779720  [25664/60000]\n",
            "loss: 0.769152  [32064/60000]\n",
            "loss: 0.854347  [38464/60000]\n",
            "loss: 0.814982  [44864/60000]\n",
            "loss: 0.837876  [51264/60000]\n",
            "loss: 0.792824  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.786043 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
        "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
        "#model.eval()\n",
        "\n",
        "torch.save(model, 'model.pth')\n",
        "\n",
        "model = torch.load('model.pth', weights_only=False),"
      ],
      "metadata": {
        "id": "jMipoQtUTaTA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
        "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "hheuNMAwWHJb",
        "outputId": "915c4bf7-4b1e-4ec6-c07a-550b28482a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "[Errno 2] No such file or directory: 'data/PennFudanPed/PNGImages/FudanPed00046.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a624e52ef273>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/PennFudanPed/PNGImages/FudanPed00046.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: 'data/PennFudanPed/PNGImages/FudanPed00046.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "XriQU_cQJNOY",
        "outputId": "5a192209-de1b-4886-f654-89dbafa8a842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7fae025b3890>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-30gjvtCkr5"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Quickstart\n",
        "==========\n",
        "\n",
        "This section runs through the API for common tasks in machine learning.\n",
        "Refer to the links in each section to dive deeper.\n",
        "\n",
        "Working with data\n",
        "-----------------\n",
        "\n",
        "PyTorch has two [primitives to work with\n",
        "data](https://pytorch.org/docs/stable/data.html):\n",
        "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
        "stores the samples and their corresponding labels, and `DataLoader`\n",
        "wraps an iterable around the `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JizgCMmxCkr9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix1dlzDYCkr-"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as\n",
        "[TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
        "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
        "include datasets. For this tutorial, we will be using a TorchVision\n",
        "dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many\n",
        "real-world vision data like CIFAR, COCO ([full list\n",
        "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
        "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
        "includes two arguments: `transform` and `target_transform` to modify the\n",
        "samples and labels respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr5Q9E-fCkr_"
      },
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBlU9JRfCkr_"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
        "iterable over our dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading. Here we define a batch size of\n",
        "64, i.e. each element in the dataloader iterable will return a batch of\n",
        "64 features and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9mweyphCkr_"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_thStJV5CksA"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4chx7VCksB"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU2Y3QENCksB"
      },
      "source": [
        "Creating Models\n",
        "===============\n",
        "\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "We define the layers of the network in the `__init__` function and\n",
        "specify how data will pass through the network in the `forward`\n",
        "function. To accelerate operations in the neural network, we move it to\n",
        "the\n",
        "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
        "available, we will use it. Otherwise, we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew-DaC28CksC"
      },
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdSELiiyCksC"
      },
      "source": [
        "Read more about [building neural networks in\n",
        "PyTorch](buildmodel_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfWPVTSCksC"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz5ZyboWCksD"
      },
      "source": [
        "Optimizing the Model Parameters\n",
        "===============================\n",
        "\n",
        "To train a model, we need a [loss\n",
        "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
        "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBxVe34jCksD"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMg37L12CksD"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training\n",
        "dataset (fed to it in batches), and backpropagates the prediction error\n",
        "to adjust the model\\'s parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRxtb8X1CksD"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-FysFFUCksD"
      },
      "source": [
        "We also check the model\\'s performance against the test dataset to\n",
        "ensure it is learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVZOWnLqCksE"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ph2DMmZCksE"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*).\n",
        "During each epoch, the model learns parameters to make better\n",
        "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
        "we\\'d like to see the accuracy increase and the loss decrease with every\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4RKX9IECksF"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiikY4ACksF"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYvtdnCrCksG"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dcZnqBoCksG"
      },
      "source": [
        "Saving Models\n",
        "=============\n",
        "\n",
        "A common way to save a model is to serialize the internal state\n",
        "dictionary (containing the model parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl0vX9cyCksG"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWJznrPCksG"
      },
      "source": [
        "Loading Models\n",
        "==============\n",
        "\n",
        "The process for loading a model includes re-creating the model structure\n",
        "and loading the state dictionary into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYzc-FHACksG"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwBM5fNdCksH"
      },
      "source": [
        "This model can now be used to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMbdJg57CksH"
      },
      "outputs": [],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGwCX8joCksH"
      },
      "source": [
        "Read more about [Saving & Loading your\n",
        "model](saveloadrun_tutorial.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}